{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "from helpers.helper_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombing_Operations = spark.read.json(\"data_templates/Bombing_Operations.json.gz\")\n",
    "Aircraft_Glossary = spark.read.json(\"data_templates/Aircraft_Glossary.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MissionDate</th>\n",
       "      <th>ContryFlyingMission</th>\n",
       "      <th>MissionsCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1965-10-01</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1965-10-02</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1965-10-03</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1965-10-04</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965-10-05</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MissionDate       ContryFlyingMission  MissionsCount\n",
       "0  1965-10-01  UNITED STATES OF AMERICA            447\n",
       "1  1965-10-02  UNITED STATES OF AMERICA            652\n",
       "2  1965-10-03  UNITED STATES OF AMERICA            608\n",
       "3  1965-10-04  UNITED STATES OF AMERICA            532\n",
       "4  1965-10-05  UNITED STATES OF AMERICA            697"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missions_countries = Bombing_Operations.selectExpr([\"to_date(MissionDate) as MissionDate\", \"ContryFlyingMission\"])\n",
    "missions_by_date = missions_countries\\\n",
    "                    .groupBy([\"MissionDate\", \"ContryFlyingMission\"])\\\n",
    "                    .agg(count(\"*\").alias(\"MissionsCount\"))\\\n",
    "                    .sort(asc(\"MissionDate\")).toPandas()\n",
    "missions_by_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, missions in missions_by_date.groupby(\"ContryFlyingMission\"): \n",
    "    plt.plot(missions[\"MissionDate\"], missions[\"MissionsCount\"], label=country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map/Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locations = jun_29_operations.rdd.map(lambda row: (row.TakeoffLocation, 1))\n",
    "all_locations.take(3)\n",
    "locations_counts_rdd = all_locations.reduceByKey(lambda a, b: a+b).sortBy(lambda r: -r[1])\n",
    "locations_counts_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert RDD to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_counts_with_schema = locations_counts_rdd.map(lambda r: Row(TakeoffLocation=r[0], MissionsCount=r[1]))\n",
    "locations_counts = spark.createDataFrame(locations_counts_with_schema)\n",
    "locations_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer, stopwords_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode words from list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words in a single dataframe\n",
    "all_words = reddit_with_tokens.select(explode(\"words\").alias(\"word\"))\n",
    "# group by, sort and limit to 50k \n",
    "top50k = all_words.groupBy(\"word\").agg(count(\"*\").alias(\"total\")).sort(col(\"total\").desc()).limit(50000)\n",
    "\n",
    "top50k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a representation of subreddits and number of 50k words in this subreddit: 1 sureddit - all word in it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_50k = filtered_tokens.rdd.map(lambda r: (r.subreddit, [r.word])).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackard similarity heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: similarity is computed 2 times! It can be optimized\n",
    "similarity = []\n",
    "for sr1 in subreddit_50k:\n",
    "    for sr2 in subreddit_50k:\n",
    "        similarity.append((sr1[0], sr2[0], jaccard_similarity(sr1[1], sr2[1])))\n",
    "\n",
    "\n",
    "similarity_matrix_50k_words = pd.DataFrame(similarity).pivot(index=0, columns=1, values=2)\n",
    "plot_heatmap(similarity_matrix_50k_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorized bag of words representation for each dialogue line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd=sc.textFile('data_templates/all_scripts.txt')\n",
    "\n",
    "df_rdd=df_rdd.flatMap(lambda line: line.split('\\n'))\n",
    "\n",
    "df_rdd=df_rdd.filter(lambda l: not (l.startswith('>'))).map(\\\n",
    "                        lambda line: [part.strip(' ') for part in line.split(':', 1)])\n",
    "\n",
    "df_rdd=df_rdd.map(lambda l: [l[0], ''.join([\\\n",
    "                        char if char not in EXCLUDE_CHARS else ' ' for char in l[1]]).lower()])\n",
    "\n",
    "df_rdd=df_rdd.map(lambda l: [l[0],[word.strip(' ') for word in l[1].split(' ')\\\n",
    "                        if word !=' ' and word !='' and len(word)>1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|   character|                 BoW|\n",
      "+------------+--------------------+\n",
      "|     Sheldon|[so, if, photon, ...|\n",
      "|     Leonard|[agreed, what, yo...|\n",
      "|     Sheldon|[there, no, point...|\n",
      "|     Leonard|        [excuse, me]|\n",
      "|Receptionist|          [hang, on]|\n",
      "|     Leonard|[one, across, is,...|\n",
      "|Receptionist|    [can, help, you]|\n",
      "|     Leonard|[yes, um, is, thi...|\n",
      "|Receptionist|[if, you, have, t...|\n",
      "|     Sheldon|[think, this, is,...|\n",
      "|Receptionist|  [fill, these, out]|\n",
      "|     Leonard|[thank, you, we, ...|\n",
      "|Receptionist|[oh, take, your, ...|\n",
      "|     Sheldon|[leonard, don, th...|\n",
      "|     Leonard|[what, are, you, ...|\n",
      "|     Sheldon|[no, we, are, com...|\n",
      "|     Leonard|[sheldon, this, w...|\n",
      "|     Sheldon|[know, and, do, y...|\n",
      "|     Leonard|[sure, she, ll, s...|\n",
      "|     Sheldon|            [wouldn]|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lines= spark.createDataFrame(df_rdd,('character','BoW'))\n",
    "df_lines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         character|                 BoW|\n",
      "+------------------+--------------------+\n",
      "|           Sheldon|[[photon, 1], [is...|\n",
      "|           Leonard|[[point, 43], [ex...|\n",
      "|            Howard|[[till, 3], [this...|\n",
      "|             Voice|[[lost, 1], [was,...|\n",
      "|               Man|[[in, 8], [am, 3]...|\n",
      "|            Lesley|[[leonard, 7], [h...|\n",
      "|    Howardâ€™s phone|[[say, 2], [call,...|\n",
      "|            Waiter|[[oh, 1], [where,...|\n",
      "|          Together|[[yes, 1], [at, 1...|\n",
      "|              Toby|[[was, 3], [more,...|\n",
      "|            Leslie|[[leonard, 10], [...|\n",
      "|             Missy|[[nice, 2], [meet...|\n",
      "|             Woman|[[have, 1], [own,...|\n",
      "|             Nurse|[[fill, 3], [this...|\n",
      "|       Gablehauser|[[am, 1], [very, ...|\n",
      "|           Warrior|[[again, 1], [leo...|\n",
      "|Leonard and Howard|[[don, 4], [oh, 1...|\n",
      "|          DMV Lady|[[take, 2], [this...|\n",
      "|             Steph|[[maybe, 2], [we,...|\n",
      "|            Kripke|[[hofstadter, 4],...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd_=df_rdd.flatMap(lambda l : [((l[0],word), 1) for word in l[1]]).reduceByKey(lambda a,b: a+b)\n",
    "df_rdd_=df_rdd_.map(lambda t: (t[0][0],[(t[0][1], t[1])])).reduceByKey(lambda a,b: a+b)\n",
    "df= spark.createDataFrame(df_rdd_,('character','BoW'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Class 1: string (nullable = true)\n",
      " |-- Class 2: string (nullable = true)\n",
      " |-- HP: string (nullable = true)\n",
      " |-- Attack: string (nullable = true)\n",
      " |-- Defense: string (nullable = true)\n",
      " |-- Sp. Atk: string (nullable = true)\n",
      " |-- Sp. Def: string (nullable = true)\n",
      " |-- Speed: string (nullable = true)\n",
      " |-- Legendary: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- First_pokemon: string (nullable = true)\n",
      " |-- Second_pokemon: string (nullable = true)\n",
      " |-- Winner: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Scaled_pokemons=spark.read.load('data_templates/pokemon.csv',format='csv',header='true')\n",
    "Scaled_combats=spark.read.load('data_templates/combats.csv',format='csv',header='true')\n",
    "\n",
    "Scaled_pokemons.printSchema()\n",
    "Scaled_combats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------------------+-------+--------+---+------+-------+-------+-------+-----+---------+\n",
      "|Winner|count(1)|pid|              Name|Class 1| Class 2| HP|Attack|Defense|Sp. Atk|Sp. Def|Speed|Legendary|\n",
      "+------+--------+---+------------------+-------+--------+---+------+-------+-------+-------+-----+---------+\n",
      "|   154|     136|154|        Aerodactyl|   Rock|  Flying| 80|   105|     65|     60|     75|  130|    FALSE|\n",
      "|   155|     127|155|   Mega Aerodactyl|   Rock|  Flying| 80|   135|     85|     70|     95|  150|    FALSE|\n",
      "|   163|     152|163|            Mewtwo|Psychic|    null|106|   110|     90|    154|     90|  130|     TRUE|\n",
      "|   214|     130|214|           Murkrow|   Dark|  Flying| 60|    85|     42|     85|     42|   91|    FALSE|\n",
      "|   249|     128|249|     Mega Houndoom|   Dark|    Fire| 75|    90|     90|    140|     90|  115|    FALSE|\n",
      "|   314|     133|314|           Slaking| Normal|    null|150|   160|    100|     95|     65|  100|    FALSE|\n",
      "|   394|     130|394|        Mega Absol|   Dark|    null| 65|   150|     60|    115|     60|  115|    FALSE|\n",
      "|   428|     134|428|           Jirachi|  Steel| Psychic|100|   100|    100|    100|    100|  100|     TRUE|\n",
      "|   432|     133|432|Deoxys Speed Forme|Psychic|    null| 50|    95|     90|     95|     90|  180|     TRUE|\n",
      "|   438|     136|438|         Infernape|   Fire|Fighting| 76|   104|     71|    104|     71|  108|    FALSE|\n",
      "+------+--------+---+------------------+-------+--------+---+------+-------+-------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aggregate number of victories, sort by number and take top 10\n",
    "Victory_numbers=Scaled_combats.groupby('Winner').agg(count('*')).sort(desc('count(1)')).limit(10)\n",
    "\n",
    "#Join to datasets to get the names\n",
    "Pokemons_names_victories=Victory_numbers.join(Scaled_pokemons,Scaled_pokemons.pid==Victory_numbers.Winner)\n",
    "\n",
    "Pokemons_names_victories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mewtwo', 'Aerodactyl', 'Infernape', 'Jirachi', 'Slaking', 'Deoxys Speed Forme', 'Murkrow', 'Mega Absol', 'Mega Houndoom', 'Mega Aerodactyl']\n"
     ]
    }
   ],
   "source": [
    "Names=Pokemons_names_victories.sort(desc('count(1)')).select('Name').collect()\n",
    "Names_list=[name.Name for name in Names]\n",
    "print(Names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
