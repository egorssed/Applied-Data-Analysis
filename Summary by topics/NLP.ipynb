{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#The data\n",
    "corpus_root = 'books/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09 - Handling text 1\n",
    "\n",
    "\n",
    "Implementing the natural language processing pipeline  \n",
    "Solving four typical language processing tasks:  \n",
    "Sentiment analysis  \n",
    "Document classification  \n",
    "Topic detection  \n",
    "Semantic analysis  \n",
    "Part 1  \n",
    "\n",
    "load the books    \n",
    "Remove the new lines  \n",
    "put in raw text, get a Spacy object  \n",
    "create our own NLP pipeline with Spacy  \n",
    "Step 1: Sentence splitting  \n",
    "Step 2: Tokenization  \n",
    "Step 3: Part of speech tagging  \n",
    "Step 4: Named entity recognition  \n",
    "Step 5: Removing stop words  \n",
    "Step 6: Lemmatization  \n",
    "Step 7: Chunking (shallow parsing)  \n",
    "Step 8: Dependancy parsing  \n",
    "Counting word occurences  \n",
    "The NLP pipeline with Spacy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdd/anaconda3/envs/adaexam/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "example = 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.'\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdd/anaconda3/envs/adaexam/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sent = []\n",
    "#iterate through the sentences, get polarity scores, choose a value\n",
    "[positive_sent.append(analyzer.polarity_scores(sent.text)['pos']) for sent in doc.sents]\n",
    "negative_sent = []\n",
    "[negative_sent.append(analyzer.polarity_scores(sent.text)['neg']) for sent in doc.sents]\n",
    "#plt.hist(negative_sent,bins=15)\n",
    "total_sent = []\n",
    "[total_sent.append(analyzer.polarity_scores(sent.text)['compound']) for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our corpus via NLTK this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "our_books = PlaintextCorpusReader(corpus_root, '.*.txt')\n",
    "print(our_books.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment the books into equally long chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yield successive n-sized chunks from l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "\"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "for i in range(0, len(l), n):\n",
    "yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dictionary of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id = {f:n for n,f in enumerate(our_books.fileids())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment the books into equally long chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list()\n",
    "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
    "limit = 500 # how many chunks total\n",
    "size = 50 # how many sentences per chunk/page\n",
    "for f in our_books.fileids():\n",
    "sentences = our_books.sents(f)\n",
    "print(f,\":\")\n",
    "print('Number of sentences:',len(sentences))\n",
    "# create chunks\n",
    "chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
    "chs = list()\n",
    "# regroup so to have a list of chunks which are strings\n",
    "for c in chunks_of_sents:\n",
    "grouped_chunk = list()\n",
    "for s in c:\n",
    "grouped_chunk.extend(s)\n",
    "chs.append(\" \".join(grouped_chunk))\n",
    "print(\"Number of chunks:\",len(chs),'\\n')\n",
    "# regroup so to have a list of chunks which are strings\n",
    "for c in chunks_of_sents:\n",
    "grouped_chunk = list()\n",
    "for s in c:\n",
    "grouped_chunk.extend(s)\n",
    "chs.append(\" \".join(grouped_chunk))\n",
    "print(\"Number of chunks:\",len(chs),'\\n')\n",
    "# filter to the limit, to have the same number of chunks per book\n",
    "chunks.extend(chs[:limit])\n",
    "chunk_class.extend([book_id[f] for _ in range(len(chs[:limit]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the chunks with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "#initialize and specify minumum number of occurences to avoid untractable number of features\n",
    "#vectorizer = CountVectorizer(min_df = 2) if we want high frequency\n",
    "#create bag of words features\n",
    "X = vectorizer.fit_transform(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the regularized logistic regression and find c using cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask and convert to int Frankenstein\n",
    "Y = np.array(chunk_class) == 1\n",
    "Y = Y.astype(int)  \n",
    "\n",
    "#shuffle the data\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "#split into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "accs = []\n",
    "\n",
    "#the grid of regularization parameter \n",
    "grid = [0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "for c in grid:\n",
    "    \n",
    "    #initialize the classifier\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',C = c)\n",
    "    \n",
    "    #crossvalidate\n",
    "    scores = cross_val_score(clf, X_train,Y_train, cv=10)\n",
    "    accs.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -20)[-20:]\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word emdeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list((nlp(example).vector)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "processed_docs = list()\n",
    "for doc in nlp.pipe(chunks, n_threads=5, batch_size=10):\n",
    "\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams too\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary representation of the documents, and filter out frequent and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words representation of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model = LdaMulticore(corpus=corpus, num_topics=4, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment of cluster\n",
    "sent_to_cluster = list()\n",
    "for n,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        cluster = max(model[doc],key=lambda x:x[1])\n",
    "        sent_to_cluster.append(cluster[0])\n",
    "# accuracy\n",
    "from collections import Counter\n",
    "for book, cluster in book_id.items():\n",
    "    assignments = list()\n",
    "    for real,given in zip(chunk_class,sent_to_cluster):\n",
    "        if real == cluster:\n",
    "            assignments.append(given)\n",
    "    most_common,num_most_common = Counter(assignments).most_common(1)[0] # 4, 6 times\n",
    "    print(book,\":\",most_common,\"-\",num_most_common)\n",
    "    print(\"Accuracy:\",num_most_common/limit)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic analysis based on lexical categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(books[3])\n",
    "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evolution of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0,len(doc.text),150000)\n",
    "love = []\n",
    "pain = []\n",
    "beauty = []\n",
    "affection = []\n",
    "\n",
    "\n",
    "for cnt,i in enumerate(bins[:-1]):\n",
    "    empath_features = lexicon.analyze(doc.text[bins[cnt]:bins[cnt+1]],\n",
    "                                      categories = [\"love\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)\n",
    "    love.append(empath_features[\"love\"])\n",
    "    pain.append(empath_features[\"pain\"])\n",
    "    beauty.append(empath_features[\"beauty\"])\n",
    "    affection.append(empath_features[\"affection\"])\n",
    "plt.plot(love,label = \"love\")\n",
    "plt.plot(beauty, label = \"beauty\")\n",
    "plt.plot(affection, label = \"affection\")\n",
    "plt.plot(pain,label = \"pain\")\n",
    "\n",
    "plt.xlabel(\"progression in the book\")\n",
    "plt.ylabel(\"frequency of a category\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create custom categories based on seed terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model can be: reddit, nytimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling text 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading txt to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = \"\"\n",
    "episode = \"\"\n",
    "scene = \"\"\n",
    "data = []\n",
    "with open(\"data/all_scripts.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line[:-1]\n",
    "        if line.startswith(\">> \"):\n",
    "            season = int(line[10:12])\n",
    "            episode = line[3:]\n",
    "            continue\n",
    "        if line.startswith(\"> \"):\n",
    "            scene = line[2:]\n",
    "            continue\n",
    "        character, line = line.split(\": \", 1)\n",
    "        data.append([season, episode, scene, character, line])\n",
    "lines = pd.DataFrame(data, columns=[\"Season\", \"Episode\", \"Scene\", \"Character\", \"Line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace punctuation marks and lowercase all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    for char in EXCLUDE_CHARS:\n",
    "        line = line.replace(char, ' ')\n",
    "    return line.lower()\n",
    "lines[\"Line\"] = lines[\"Line\"].apply(clean_line)\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count  and plot word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_frequency = pd.concat([pd.Series(row['Line'].split(' ')) for _, row in lines.iterrows()]).reset_index()\n",
    "corpus_frequency.columns = [\"Frequency\", \"Word\"]\n",
    "corpus_frequency = corpus_frequency.groupby(\"Word\").count()\n",
    "\n",
    "corpus_frequency.plot.hist(by=\"Frequency\", bins=100, title=\"Frequency histogram\")\n",
    "corpus_frequency.plot.hist(by=\"Frequency\", loglog=True, bins=np.logspace(0, 6, 100),\n",
    "                           title=\"Frequency histogram (loglog scale)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of word per character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[\"Words\"] = lines[\"Line\"].apply(lambda x: len(x.split(' ')))\n",
    "words_per_char = lines.groupby(\"Character\").sum()[\"Words\"]\n",
    "words_per_char[recurrent_chars.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer with stopwords and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"helpers/stopwords.txt\") as f:\n",
    "    stop_words = list(map(lambda x: x[:-1], f.readlines()))\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=simple_tokeniser, min_df=2)\n",
    "train_vectors = tfidf.fit_transform(train_set[\"Line\"])\n",
    "test_vectors = tfidf.transform(test_set[\"Line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the set of all words in the training set that are only uttered by Sheldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_for_chars = pd.concat([pd.Series(row[\"Character\"], row['Line'].split(' '))\n",
    "                             for _, row in lines.iterrows()]).reset_index()\n",
    "words_for_chars.columns = [\"Word\", \"Character\"]\n",
    "\n",
    "words_for_chars = words_for_chars.groupby(\"Word\")[\"Character\"].apply(set)\n",
    "sheldon_words = words_for_chars[words_for_chars.apply(lambda x: (\"Sheldon\" in x) and (len(x) == 1))].index\n",
    "\n",
    "def contains_sheldon_words(line):\n",
    "    for word in sheldon_words:\n",
    "        if word in line:\n",
    "            return True\n",
    "    return False\n",
    "test_pred = test_set[\"Line\"].apply(contains_sheldon_words)\n",
    "test_true = test_set[\"Character\"] == \"Sheldon\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=25)\n",
    "train_svd = svd.fit_transform(train_vectors)\n",
    "test_svd = svd.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionCV(cv=10)\n",
    "train_labels = train_set[\"Character\"] == \"Sheldon\"\n",
    "model.fit(train_svd, train_labels)\n",
    "test_pred = model.predict(test_svd)\n",
    "train_pred = model.predict(train_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(test, pred, positive=1):\n",
    "    negative = 0 if positive == 1 else 1\n",
    "    cm = np.zeros((2,2))\n",
    "    test = test.values\n",
    "    cm[0,0] = np.logical_and(pred == positive, test == positive).sum()\n",
    "    cm[0,1] = np.logical_and(pred == positive, test == negative).sum()\n",
    "    cm[1,0] = np.logical_and(pred == negative, test == positive).sum()\n",
    "    cm[1,1] = np.logical_and(pred == negative, test == negative).sum()\n",
    "    df = pd.DataFrame(cm.astype(int), columns=[\"Positive\", \"Negative\"])\n",
    "    df.index = [\"Positive Prediction\", \"Negative Prediction\"]\n",
    "    return df\n",
    "\n",
    "def accuracy(confusion_matrix):\n",
    "    return (confusion_matrix[0,0] + confusion_matrix[1,1]) / confusion_matrix.sum()\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    if (confusion_matrix[0,:].sum() == 0):\n",
    "        return 1\n",
    "    return confusion_matrix[0,0] / confusion_matrix[0,:].sum()\n",
    "\n",
    "def recall(confusion_matrix):\n",
    "    if (confusion_matrix[:,0].sum() == 0):\n",
    "        return 1\n",
    "    return confusion_matrix[0,0] / confusion_matrix[:,0].sum()\n",
    "\n",
    "def fscore(confusion_matrix):\n",
    "    p = precision(confusion_matrix)\n",
    "    r = recall(confusion_matrix)\n",
    "    return 2 * p * r / (p+r)\n",
    "\n",
    "def stats(confusion_matrix):\n",
    "    confusion_matrix = confusion_matrix.values\n",
    "    return {\"accuracy\": accuracy(confusion_matrix), \"precision\":precision(confusion_matrix),\n",
    "            \"recall\": recall(confusion_matrix), \"fscore\": fscore(confusion_matrix)}\n",
    "\n",
    "print(\"Statistics for class 1 on train set:\\n\", stats(confusion_matrix(train_labels, train_pred, positive=1)))\n",
    "print(\"Statistics for class 0 on train set:\\n\", stats(confusion_matrix(train_labels, train_pred, positive=0)))\n",
    "print(\"Statistics for class 1 on test set:\\n\", stats(confusion_matrix(test_true, test_pred, positive=1)))\n",
    "print(\"Statistics for class 0 on test set:\\n\", stats(confusion_matrix(test_true, test_pred, positive=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
